---
output:
  html_document: default
  pdf_document: default
---

# Bayesian Statistics

Statistics is a tool to learn and make decisions concerning a system by measurement and inference.
Data obtained by measurement of the system are stochastic, meaning the process generating the data exhibits natural random variation.
This variation can be described by specifying probabilistic models -- mathematical formulations of the system.
We can characterise the properties of the probability models via a process of statistical inference.
The inference is conditional on both the model assumptions and the observed data.

In frequentist statistics, the model usually comprises a family of *sampling densities* denoted by $f(y|\theta)$ governed by a fixed but unknown *parameter* $\theta$ taking values in a set $\Theta$. 
As a function of the parameter for a given observation $\tilde y$, this is the  *likelihood function* $f(\tilde y|\theta)$. 
Inferences about $\theta$ are made on the basis of this likelihood function that encodes our information about the value of $\theta$ given the observation $\tilde y$. 
These inferences can then be used to draw conclusions about the system under study in terms of its relation to the model.

In Bayesian inference, in addition to a probability model for the data generating process, a probability model is also specified for other unknown quantities.
This additional probability model reflects our knowledge and uncertainty in the model parameters.
All model uncertainty is measured in terms of probability and inference proceeds by use of Bayes theorem, which tells us 
$$
p(\theta|\tilde y) = \frac{f(\tilde y|\theta)p(\theta)}{m(\tilde y)}.
$$
That is, our prior knowledge of $\theta$, encoded by some probability model $p(\theta)$, is updated according to the the likelihood from the observation $\tilde y$ to yield a *posterior* distribution.
The posterior, denoted $p(\theta|\tilde y)$, reflects our updated state of belief.
The denominator $m(\tilde y)$, which is the marginal probability of the data, is a normalising constant that ensures the posterior is a valid probability distribution.
Thus, in the Bayesian framework, statistical inferences are reduced to relevant summaries of the posterior.

Historically, the majority of clinical trials have used frequentist methods that ask the question: "What would I expect to observe if there is no difference in the effectiveness of the two treatments being evaluated?"
Many people find this framing to be both esoteric and unintuitive.
Contrastingly, Bayesian statistics concerns itself with a more relevant question, namely "Given what I have observed, what is the probability that one treatment is better than the other?" 
 
**NOTE: I think the example below is useful for explaining how Bayes theorem works, but not necessarily for how to thnk about Bayesian statistics. Bayes theorem is true for everyone, but using Bayes theorem for everything is Bayesian. This same example would equally be applied by a frequentist because the prevalence reflects the long run frequency of the outcome in the population. Trying to think of a more "Bayesian" example which is still simple enough?.**

Some intuition into the usefulness of the Bayesian perspective can be gained through applying Bayes theorem to the simple problem of how one should interpret a positive test result for a particular infection.
Here we cast $\theta$ as a hypothesis of infection and $\tilde y$ as a test result and suppose that the sensitivity and specificity of the test are 99% and 95% respectively.
The sensitivity and specificity measures tell us the probability that the test is positive given that an infection is detected is 99% and the probability that the test is negative given that an infection is absent is 95% respectively.
However, we are interested in the probability that we have the infection given the test is positive. 
Assuming that the prevelance of this bacterial infection in the community is 1% and applying Bayes therom we arrive at an answer that is a little lower than 20%.  
$$
\begin{eqnarray} 
p(\theta|\tilde y) &=& \frac{0.99 \times 0.01}{0.99 \times 0.01 + 0.05 \times 0.99} \\ &\approx& 0.17. 
\end{eqnarray}
$$

With very few exceptions, we possess prior knowledge about the system under examination.
Sometimes this knowledge is very imprecise, but we will know *something*.
In the above example we assumed that we had information on the population prevalence for infection. 
In an efficacy study for a new treatment, prior knowledge may come from pilot studies, previous clinical trials investigating similar treatments, or even expert knowledge on the biology of the illness.
Bayesian methods enable us to incorporate this existing knowledge (or belief) into our current study.
As a consequence, we may find values for the parameters of interest that are closer to the true value.
However, this strength can also lead to criticism due to the perceived negative implications of the subjective choice of priors. 
Analysts must take care in formulating appropriate priors, exploring a range of skeptical, pessimistic and optimistic evidence. 
One approach for exploring the implications of a given prior is to assess the plausibility of data generated via the *prior predictive distribution*, which is the (modeled) distribution of $y$ marginalized over the prior
$$
m(y') = \int_{\Theta} f(y'|\theta) p(\theta)d\theta.
$$
Bayesian models are generative.
The *posterior predictive* distribution gives the distribution of new data marginalised over the posterior, thereby accounting for the uncertainty in $\theta$
$$
m(y'|\tilde y) = \int_{\Theta} f(y'|\theta) p(\theta|\tilde y)d\theta.
$$
The *posterior predictive* reflects how we believe future data would appear.

In specifying probability models on unknown quantities such as model parameters we naturally move from interpreting probability as long-run frequencies to interpreting probability as reflecting subjective uncertainty.
Updating the posterior distribution reflects how our uncertainty in knowledge of the model parameters changes with additional observations from the system.

# How can Bayesian Statistics be used in adaptive designs?

According to the FDA, an adaptive design clinical study is a study that includes a prospectively planned opportunity for modification of one or more specified aspects of the study design and hypotheses based on analysis of data (usually interim data) from subjects in the study (*Adaptive Design Clinical Trials for Drugs and Biologics*).

Adaptive designs allow for more flexible clinical trials by modifying the design in accordance with the prospectively specified allowable changes and decision rules based on accumulating data which occur at pre-specified timings for **interim analyses**.
The Bayesian approach naturally fits with the process of sequential changes where the posterior distribution given the current available data is used to drive decisions at interim analyses.

By implementing a design which allows for modifications, the trial may be more efficient or ethical. A trial may be stopped before recruiting up to the pre-planned sample size if there is strong evidence that it makes sense to do so. 
For example, a trial might stop for futility if, given the available data, there is low probability of identifying a beneficial effect if the trial were to continue. 
Alternatively, if at an interim analysis there is already strong evidence for superiority of a particular arm, then the trial may be stopped early for success. 
These possibilities mean fewer patients are enrolled into a trial for a treatment that may have no benefits, and more patients may benefit from efficacious treatments sooner than if the trial continued until the maximum sample size before reporting results.

An adaptive trial may also be more ethical for participants enrolled into the study. 
As information accrues, there may be evidence that certain trial arms are more beneficial to patient outcomes than others. 
In a traditional design, subjects are randomised to treatments according to fixed allocations, however, in an adaptive design, the allocations may be updated at each interim analysis by increasing the probability that new subjects are allocated to the most beneficial treatments. 
This **response adaptive randomisation** means that more subjects may treated under the best treatment option than would occur in a traditional trial.
Bayesian posterior probabilities of treatment superiority provide a straight forward mechanism for updating treatment allocation ratios.